{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3821116,"sourceType":"datasetVersion","datasetId":2275641},{"sourceId":12474371,"sourceType":"datasetVersion","datasetId":7870300},{"sourceId":12474419,"sourceType":"datasetVersion","datasetId":7870340},{"sourceId":12485912,"sourceType":"datasetVersion","datasetId":7878768},{"sourceId":12487042,"sourceType":"datasetVersion","datasetId":7879625}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical, Sequence\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport albumentations as A\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\n\n# Custom Focal Loss Function\ndef categorical_focal_loss(alpha=0.25, gamma=2.0):\n    def loss(y_true, y_pred):\n        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())\n        cross_entropy = -y_true * K.log(y_pred)\n        weight = alpha * K.pow(1 - y_pred, gamma)\n        loss = weight * cross_entropy\n        return K.sum(loss, axis=1)\n    return loss\n\n# Configuration\nCONFIG = {\n    \"BATCH_SIZE\": 128,\n    \"NUM_CLASSES\": 27,\n    \"LR\": 0.0001,\n    \"EPOCHS\": 50\n}\n\n# Dataset Paths\nTRAIN_PATH = \"/kaggle/input/train-alphabet1\"\nVALIDATION_PATH = \"/kaggle/input/test-dataset1\"\nSELECTED_CLASSES = [\n    'A', 'B', 'Blank' , 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'\n]\nclass_to_idx = {cls: idx for idx, cls in enumerate(SELECTED_CLASSES)}\n\n# Albumentations Transforms\ntrain_transform = A.Compose([\n    A.OneOf([\n        A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n        A.CLAHE(clip_limit=2.0, p=0.5)\n    ], p=0.4),\n    A.HueSaturationValue(5, 10, 10, p=0.3),\n    A.ImageCompression(quality_lower=85, quality_upper=100, p=0.2),\n    A.Equalize(p=0.1),\n    A.ShiftScaleRotate(0.02, 0.02, 9, border_mode=0, p=0.5),\n    A.MotionBlur(blur_limit=(3, 5), p=0.1),\n])\n\nval_transform = A.Compose([])  # No augmentation for validation\n\nMAX_IMAGES_PER_CLASS = 750\n\n# Load Training Data\ntrain_image_paths, train_labels = [], []\nfor class_name in SELECTED_CLASSES:\n    folder = os.path.join(TRAIN_PATH, class_name)\n    all_images = glob(os.path.join(folder, '*.png'))\n    selected_images = all_images[:MAX_IMAGES_PER_CLASS] if len(all_images) > MAX_IMAGES_PER_CLASS else all_images\n    for img_file in selected_images:\n        train_image_paths.append(img_file)\n        train_labels.append(class_to_idx[class_name])\n\n# Load Validation Data\nval_image_paths, val_labels = [], []\nfor class_name in SELECTED_CLASSES:\n    folder = os.path.join(VALIDATION_PATH, class_name)\n    all_images = glob(os.path.join(folder, '*.png'))\n    selected_images = all_images[:MAX_IMAGES_PER_CLASS] if len(all_images) > MAX_IMAGES_PER_CLASS else all_images\n    for img_file in selected_images:\n        val_image_paths.append(img_file)\n        val_labels.append(class_to_idx[class_name])\n\n\n# Data Generator\nclass AlbumentationsGenerator(Sequence):\n    def __init__(self, image_paths, labels, batch_size, transform, num_classes, augmentations_per_image=1):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.batch_size = batch_size\n        self.transform = transform\n        self.num_classes = num_classes\n        self.augmentations_per_image = augmentations_per_image\n\n    def __len__(self):\n        return int(np.ceil(len(self.image_paths) * self.augmentations_per_image / self.batch_size))\n\n    def __getitem__(self, idx):\n        start = idx * self.batch_size // self.augmentations_per_image\n        end = (idx + 1) * self.batch_size // self.augmentations_per_image\n        batch_paths = self.image_paths[start:end]\n        batch_labels = self.labels[start:end]\n\n        images, labels = [], []\n        for img_path, label in zip(batch_paths, batch_labels):\n            image = cv2.imread(img_path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            for _ in range(self.augmentations_per_image):\n                augmented = self.transform(image=image)['image']\n                images.append(augmented / 255.0)\n                labels.append(label)\n\n        return np.array(images, dtype=np.float32), to_categorical(labels, num_classes=self.num_classes)\n\n# Input shape\nsample_image = cv2.imread(train_image_paths[0])\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\ninput_shape = sample_image.shape\n\n# Generators\ntrain_gen = AlbumentationsGenerator(\n    train_image_paths, train_labels,\n    CONFIG['BATCH_SIZE'], train_transform,\n    CONFIG['NUM_CLASSES'], augmentations_per_image=5\n)\n\nval_gen = AlbumentationsGenerator(\n    val_image_paths, val_labels,\n    CONFIG['BATCH_SIZE'], val_transform,\n    CONFIG['NUM_CLASSES'], augmentations_per_image=1\n)\n\n# Model Architecture\nbase_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\nbase_model.trainable = False\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.3)(x)\noutput = Dense(CONFIG['NUM_CLASSES'], activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=output)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(CONFIG['LR']),\n    loss=categorical_focal_loss(alpha=0.25, gamma=2.0),\n    metrics=['accuracy']\n)\n\n# Callbacks\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n]\n\n# Train the Model\nmodel.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=CONFIG['EPOCHS'],\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T12:49:54.963025Z","iopub.execute_input":"2025-07-16T12:49:54.963239Z","iopub.status.idle":"2025-07-16T15:31:23.086836Z","shell.execute_reply.started":"2025-07-16T12:49:54.963221Z","shell.execute_reply":"2025-07-16T15:31:23.086169Z"}},"outputs":[{"name":"stderr","text":"2025-07-16 12:49:57.971786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752670198.158079      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752670198.214168      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/tmp/ipykernel_36/79019342.py:49: UserWarning: Argument(s) 'quality_lower, quality_upper' are not valid for transform ImageCompression\n  A.ImageCompression(quality_lower=85, quality_upper=100, p=0.2),\n/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\nI0000 00:00:1752670218.535858      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752670218.536641      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1752670231.027077      97 service.cc:148] XLA service 0x7cb858002c20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1752670231.027787      97 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752670231.027808      97 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752670232.076422      97 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/789\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:04:23\u001b[0m 19s/step - accuracy: 0.0000e+00 - loss: 1.1936","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752670240.874366      97 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 630ms/step - accuracy: 0.2041 - loss: 0.6597 - val_accuracy: 0.7511 - val_loss: 0.1837 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 489ms/step - accuracy: 0.6819 - loss: 0.1992 - val_accuracy: 0.9164 - val_loss: 0.0591 - learning_rate: 1.0000e-04\nEpoch 3/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 516ms/step - accuracy: 0.8215 - loss: 0.1025 - val_accuracy: 0.9526 - val_loss: 0.0332 - learning_rate: 1.0000e-04\nEpoch 4/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 496ms/step - accuracy: 0.8796 - loss: 0.0638 - val_accuracy: 0.9674 - val_loss: 0.0206 - learning_rate: 1.0000e-04\nEpoch 5/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 508ms/step - accuracy: 0.9069 - loss: 0.0475 - val_accuracy: 0.9687 - val_loss: 0.0183 - learning_rate: 1.0000e-04\nEpoch 6/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 456ms/step - accuracy: 0.9250 - loss: 0.0366 - val_accuracy: 0.9736 - val_loss: 0.0134 - learning_rate: 1.0000e-04\nEpoch 7/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 493ms/step - accuracy: 0.9392 - loss: 0.0287 - val_accuracy: 0.9815 - val_loss: 0.0099 - learning_rate: 1.0000e-04\nEpoch 8/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 479ms/step - accuracy: 0.9510 - loss: 0.0235 - val_accuracy: 0.9831 - val_loss: 0.0082 - learning_rate: 1.0000e-04\nEpoch 9/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 492ms/step - accuracy: 0.9561 - loss: 0.0198 - val_accuracy: 0.9856 - val_loss: 0.0071 - learning_rate: 1.0000e-04\nEpoch 10/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 531ms/step - accuracy: 0.9627 - loss: 0.0167 - val_accuracy: 0.9868 - val_loss: 0.0063 - learning_rate: 1.0000e-04\nEpoch 11/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 476ms/step - accuracy: 0.9696 - loss: 0.0135 - val_accuracy: 0.9893 - val_loss: 0.0049 - learning_rate: 1.0000e-04\nEpoch 12/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 500ms/step - accuracy: 0.9723 - loss: 0.0121 - val_accuracy: 0.9897 - val_loss: 0.0054 - learning_rate: 1.0000e-04\nEpoch 13/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - accuracy: 0.9735 - loss: 0.0107\nEpoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 471ms/step - accuracy: 0.9735 - loss: 0.0107 - val_accuracy: 0.9893 - val_loss: 0.0048 - learning_rate: 1.0000e-04\nEpoch 14/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 487ms/step - accuracy: 0.9809 - loss: 0.0082 - val_accuracy: 0.9901 - val_loss: 0.0041 - learning_rate: 5.0000e-05\nEpoch 15/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 529ms/step - accuracy: 0.9809 - loss: 0.0076 - val_accuracy: 0.9909 - val_loss: 0.0037 - learning_rate: 5.0000e-05\nEpoch 16/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 500ms/step - accuracy: 0.9827 - loss: 0.0068 - val_accuracy: 0.9918 - val_loss: 0.0036 - learning_rate: 5.0000e-05\nEpoch 17/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 526ms/step - accuracy: 0.9852 - loss: 0.0063 - val_accuracy: 0.9934 - val_loss: 0.0035 - learning_rate: 5.0000e-05\nEpoch 18/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 468ms/step - accuracy: 0.9843 - loss: 0.0065 - val_accuracy: 0.9918 - val_loss: 0.0032 - learning_rate: 5.0000e-05\nEpoch 19/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 504ms/step - accuracy: 0.9856 - loss: 0.0062 - val_accuracy: 0.9934 - val_loss: 0.0032 - learning_rate: 5.0000e-05\nEpoch 20/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535ms/step - accuracy: 0.9856 - loss: 0.0058\nEpoch 20: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 554ms/step - accuracy: 0.9856 - loss: 0.0058 - val_accuracy: 0.9922 - val_loss: 0.0033 - learning_rate: 5.0000e-05\nEpoch 21/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 541ms/step - accuracy: 0.9882 - loss: 0.0048 - val_accuracy: 0.9946 - val_loss: 0.0028 - learning_rate: 2.5000e-05\nEpoch 22/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 493ms/step - accuracy: 0.9888 - loss: 0.0046 - val_accuracy: 0.9942 - val_loss: 0.0029 - learning_rate: 2.5000e-05\nEpoch 23/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536ms/step - accuracy: 0.9880 - loss: 0.0048\nEpoch 23: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 553ms/step - accuracy: 0.9880 - loss: 0.0048 - val_accuracy: 0.9942 - val_loss: 0.0029 - learning_rate: 2.5000e-05\nEpoch 24/50\n\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 526ms/step - accuracy: 0.9882 - loss: 0.0046 - val_accuracy: 0.9938 - val_loss: 0.0029 - learning_rate: 1.2500e-05\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7cb8ac518510>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"model.save(\"asl_227*224.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:41:52.785465Z","iopub.execute_input":"2025-07-16T15:41:52.785782Z","iopub.status.idle":"2025-07-16T15:41:53.102542Z","shell.execute_reply.started":"2025-07-16T15:41:52.785760Z","shell.execute_reply":"2025-07-16T15:41:53.101927Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model.save(\"asl_asment1.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T14:52:32.872121Z","iopub.execute_input":"2025-07-15T14:52:32.872409Z","iopub.status.idle":"2025-07-15T14:52:33.280284Z","shell.execute_reply.started":"2025-07-15T14:52:32.872386Z","shell.execute_reply":"2025-07-15T14:52:33.279413Z"}},"outputs":[],"execution_count":2}]}