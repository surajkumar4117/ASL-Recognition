{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079},{"sourceId":12423278,"sourceType":"datasetVersion","datasetId":7835802},{"sourceId":12428487,"sourceType":"datasetVersion","datasetId":7835774}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:03:19.923266Z","iopub.execute_input":"2025-07-11T09:03:19.923476Z","iopub.status.idle":"2025-07-11T09:05:59.117287Z","shell.execute_reply.started":"2025-07-11T09:03:19.923442Z","shell.execute_reply":"2025-07-11T09:05:59.116434Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:06:17.868256Z","iopub.execute_input":"2025-07-11T09:06:17.868649Z","iopub.status.idle":"2025-07-11T09:06:17.877880Z","shell.execute_reply.started":"2025-07-11T09:06:17.868627Z","shell.execute_reply":"2025-07-11T09:06:17.877055Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical, Sequence\nimport albumentations as A\n\n# Define Albumentations transform\ntransform = A.Compose([\n    A.OneOf([\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n        A.CLAHE(clip_limit=2.0, p=0.5)\n    ], p=0.4),\n    A.HueSaturationValue(hue_shift_limit=5, sat_shift_limit=10, val_shift_limit=10, p=0.3),\n    A.GaussNoise(var_limit=(5, 15), p=0.1),\n    A.ImageCompression(quality_lower=85, quality_upper=100, p=0.2),\n    A.Equalize(p=0.1),\n    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.02, rotate_limit=9, border_mode=0, p=0.5),\n    A.MotionBlur(blur_limit=(3, 5), p=0.1),\n    A.Resize(64, 64)\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:05:59.121047Z","iopub.execute_input":"2025-07-11T09:05:59.121260Z","iopub.status.idle":"2025-07-11T09:06:17.866815Z","shell.execute_reply.started":"2025-07-11T09:05:59.121241Z","shell.execute_reply":"2025-07-11T09:06:17.865857Z"}},"outputs":[{"name":"stderr","text":"2025-07-11 09:06:01.409599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752224761.603974      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752224761.665790      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/tmp/ipykernel_36/781083476.py:15: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(5, 15), p=0.1),\n/tmp/ipykernel_36/781083476.py:16: UserWarning: Argument(s) 'quality_lower, quality_upper' are not valid for transform ImageCompression\n  A.ImageCompression(quality_lower=85, quality_upper=100, p=0.2),\n/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from glob import glob\n\ndataset_path = \"/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\"\nclass_names = sorted(os.listdir(dataset_path))\nclass_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n\nimage_paths = []\nlabels = []\n\nfor class_name in class_names:\n    folder = os.path.join(dataset_path, class_name)\n    for img_file in glob(os.path.join(folder, \"*.jpg\")):\n        image_paths.append(img_file)\n        labels.append(class_to_idx[class_name])\n\n# Split into train/val (80/20)\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:06:22.472656Z","iopub.execute_input":"2025-07-11T09:06:22.472956Z","iopub.status.idle":"2025-07-11T09:06:22.697068Z","shell.execute_reply.started":"2025-07-11T09:06:22.472933Z","shell.execute_reply":"2025-07-11T09:06:22.696408Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class AlbumentationsGenerator(Sequence):\n    def __init__(self, image_paths, labels, batch_size, transform, num_classes, augmentations_per_image=4):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.batch_size = batch_size\n        self.transform = transform\n        self.num_classes = num_classes\n        self.augmentations_per_image = augmentations_per_image\n\n    def __len__(self):\n        return int(np.ceil(len(self.image_paths) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        images, labels = [], []\n\n        for img_path, label in zip(batch_paths, batch_labels):\n            image = cv2.imread(img_path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n            for _ in range(self.augmentations_per_image):\n                augmented = self.transform(image=image)['image']\n                images.append(augmented / 255.0)  # normalize\n                labels.append(label)\n\n        return np.array(images), to_categorical(labels, num_classes=self.num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:09:52.792715Z","iopub.execute_input":"2025-07-11T09:09:52.793006Z","iopub.status.idle":"2025-07-11T09:09:52.800235Z","shell.execute_reply.started":"2025-07-11T09:09:52.792986Z","shell.execute_reply":"2025-07-11T09:09:52.799299Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"batch_size = 64\nnum_classes = len(class_names)\n\ntrain_gen = AlbumentationsGenerator(\n    train_paths, train_labels, batch_size=batch_size,\n    transform=transform, num_classes=num_classes,\n    augmentations_per_image=9\n)\n\nval_gen = AlbumentationsGenerator(\n    val_paths, val_labels, batch_size=batch_size,\n    transform=transform, num_classes=num_classes,\n    augmentations_per_image=1  # usually no strong augmentation in validation\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:09:55.632881Z","iopub.execute_input":"2025-07-11T09:09:55.633198Z","iopub.status.idle":"2025-07-11T09:09:55.637991Z","shell.execute_reply.started":"2025-07-11T09:09:55.633174Z","shell.execute_reply":"2025-07-11T09:09:55.637273Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"CLASSES = [\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n    'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n    'del','nothing','space'\n]\n\nCONFIGURATION = {\n    \"BATCH_SIZE\": 64,\n    \"IM_SIZE\": 64,\n    \"LEARNING_RATE\": 0.001,\n    \"NUM_CLASSES\": 29,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:09:57.252356Z","iopub.execute_input":"2025-07-11T09:09:57.252623Z","iopub.status.idle":"2025-07-11T09:09:57.257322Z","shell.execute_reply.started":"2025-07-11T09:09:57.252602Z","shell.execute_reply":"2025-07-11T09:09:57.256704Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n\nmodel = Sequential()\n\n# Convolutional Block 1\nmodel.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(CONFIGURATION['IM_SIZE'], CONFIGURATION['IM_SIZE'], 3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\n# Convolutional Block 2\nmodel.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\n# Convolutional Block 3\nmodel.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\n# Convolutional Block 4\nmodel.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\n# Flatten and Dense Layers\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3))\n\n# Output Layer\nmodel.add(Dense(29, activation='softmax'))  # 29 classes: A-Z + space, del, nothing\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:09:59.002488Z","iopub.execute_input":"2025-07-11T09:09:59.002763Z","iopub.status.idle":"2025-07-11T09:09:59.229676Z","shell.execute_reply.started":"2025-07-11T09:09:59.002742Z","shell.execute_reply":"2025-07-11T09:09:59.229077Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport cv2\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'] )\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n]\n\nmodel.fit(train_gen, validation_data=val_gen, epochs=21, callbacks=callbacks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:10:01.712495Z","iopub.execute_input":"2025-07-11T09:10:01.712996Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1662s\u001b[0m 2s/step - accuracy: 0.2032 - loss: 2.6471 - val_accuracy: 0.9270 - val_loss: 0.2096 - learning_rate: 0.0010\nEpoch 2/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1280s\u001b[0m 1s/step - accuracy: 0.8926 - loss: 0.3276 - val_accuracy: 0.9860 - val_loss: 0.0499 - learning_rate: 0.0010\nEpoch 3/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1256s\u001b[0m 1s/step - accuracy: 0.9532 - loss: 0.1685 - val_accuracy: 0.9930 - val_loss: 0.0282 - learning_rate: 0.0010\nEpoch 4/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1217s\u001b[0m 1s/step - accuracy: 0.9690 - loss: 0.1175 - val_accuracy: 0.9961 - val_loss: 0.0185 - learning_rate: 0.0010\nEpoch 5/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1202s\u001b[0m 1s/step - accuracy: 0.9745 - loss: 0.0983 - val_accuracy: 0.9967 - val_loss: 0.0146 - learning_rate: 0.0010\nEpoch 6/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1201s\u001b[0m 1s/step - accuracy: 0.9776 - loss: 0.0890 - val_accuracy: 0.9961 - val_loss: 0.0181 - learning_rate: 0.0010\nEpoch 7/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1204s\u001b[0m 1s/step - accuracy: 0.9800 - loss: 0.0797 - val_accuracy: 0.9971 - val_loss: 0.0131 - learning_rate: 0.0010\nEpoch 8/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1199s\u001b[0m 1s/step - accuracy: 0.9818 - loss: 0.0732 - val_accuracy: 0.9974 - val_loss: 0.0114 - learning_rate: 0.0010\nEpoch 9/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1160s\u001b[0m 1s/step - accuracy: 0.9834 - loss: 0.0669 - val_accuracy: 0.9979 - val_loss: 0.0098 - learning_rate: 0.0010\nEpoch 10/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1225s\u001b[0m 1s/step - accuracy: 0.9835 - loss: 0.0679 - val_accuracy: 0.9974 - val_loss: 0.0109 - learning_rate: 0.0010\nEpoch 11/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9853 - loss: 0.0615\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1175s\u001b[0m 1s/step - accuracy: 0.9853 - loss: 0.0615 - val_accuracy: 0.9969 - val_loss: 0.0137 - learning_rate: 0.0010\nEpoch 12/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1238s\u001b[0m 1s/step - accuracy: 0.9887 - loss: 0.0473 - val_accuracy: 0.9987 - val_loss: 0.0057 - learning_rate: 5.0000e-04\nEpoch 13/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1222s\u001b[0m 1s/step - accuracy: 0.9904 - loss: 0.0400 - val_accuracy: 0.9983 - val_loss: 0.0068 - learning_rate: 5.0000e-04\nEpoch 14/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9911 - loss: 0.0370\nEpoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1217s\u001b[0m 1s/step - accuracy: 0.9911 - loss: 0.0370 - val_accuracy: 0.9989 - val_loss: 0.0059 - learning_rate: 5.0000e-04\nEpoch 15/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1278s\u001b[0m 1s/step - accuracy: 0.9918 - loss: 0.0337 - val_accuracy: 0.9987 - val_loss: 0.0047 - learning_rate: 2.5000e-04\nEpoch 16/21\n\u001b[1m1088/1088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1246s\u001b[0m 1s/step - accuracy: 0.9932 - loss: 0.0275 - val_accuracy: 0.9990 - val_loss: 0.0044 - learning_rate: 2.5000e-04\nEpoch 17/21\n\u001b[1m 142/1088\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:36\u001b[0m 1s/step - accuracy: 0.9934 - loss: 0.0255","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}